{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification of neural network from iamtrask. Source of initial script: [here](http://iamtrask.github.io/2015/07/12/basic-python-network/). His book: [here](https://www.manning.com/books/grokking-deep-learning). <br>\n",
    "This updates his code from Python 2 to 3 (i.e. simply change xrange function to range) <br>\n",
    "Adds extra comments to explain the architecture and calculations. <br>\n",
    "Modifies weight initialisation XXX <br>\n",
    "Change from Sigmoid Transformation to ReLU XXX <br>\n",
    "Modify optimisation method XXX <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature values (4 instances, 3 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels (4 instances, 1 label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Y shape: (4, 1)\n",
      "\n",
      "Y (labels): [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "Data type for Y output: int32\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "print()\n",
    "print(\"Y shape: {}\".format(y.shape))\n",
    "print()\n",
    "print(\"Y (labels): {}\".format(y))\n",
    "print()\n",
    "print(\"Data type for Y output: {}\".format(y.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random initialise weights (shape 3,4) (i.e. # weights each Features X Instances) -- uniform distribution centered on zero.\n",
    "#### Key point: dense network - the size of weight matrix is obvious when you just sketch out the architecture and draw lines between the densely connected layers - doesn't matter how many instances you have the size of matrix stays the same - independent of number of instances obviously - the full weight matrix gets multiplied by every instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights for first layer: (3, 4)\n",
      "\n",
      "The weights for first layer: [[-0.9708614  -0.67080291  0.29191409 -0.89019752]\n",
      " [-0.25176646  0.23102564  0.5872154  -0.33048972]\n",
      " [ 0.399518   -0.96349127 -0.08503958 -0.85598729]]\n",
      "\n",
      "Smallest weight: -0.9708613993238386\n",
      "\n",
      "Biggest weight: 0.587215401629215\n",
      "\n",
      "Mean weight: -0.29241358627153474\n"
     ]
    }
   ],
   "source": [
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "\n",
    "print(\"Shape of weights for first layer: {}\".format(syn0.shape))\n",
    "print()\n",
    "print(\"The weights for first layer: {}\".format(syn0))\n",
    "print()\n",
    "print(\"Smallest weight: {}\".format(np.min(syn0)))\n",
    "print()\n",
    "print(\"Biggest weight: {}\".format(np.max(syn0)))\n",
    "print()\n",
    "print(\"Mean weight: {}\".format(np.mean(syn0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random initialise weights (shape 4,1) for hidden layer transition to output (i.e. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights for hidden layer to output:\n",
      " (4, 1)\n",
      "\n",
      "The weights for hidden layer to output:\n",
      " [[0.56511337]\n",
      " [0.85871653]\n",
      " [0.82655379]\n",
      " [0.48099302]]\n",
      "\n",
      "Smallest weight: 0.4809930225597252\n",
      "\n",
      "Biggest weight: 0.8587165254354729\n",
      "\n",
      "Mean weight: 0.6828441771932812\n"
     ]
    }
   ],
   "source": [
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "print(\"Shape of weights for hidden layer to output:\\n {}\".format(syn1.shape))\n",
    "print()\n",
    "print(\"The weights for hidden layer to output:\\n {}\".format(syn1))\n",
    "print()\n",
    "print(\"Smallest weight: {}\".format(np.min(syn1)))\n",
    "print()\n",
    "print(\"Biggest weight: {}\".format(np.max(syn1)))\n",
    "print()\n",
    "print(\"Mean weight: {}\".format(np.mean(syn1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate values of nodes in Hidden Layer (i.e. forward prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First version of node values in Hidden Layer: \n",
      "  \n",
      "[[0.59857185 0.27617973 0.47875291 0.2981784 ]\n",
      " [0.53687083 0.32465389 0.62297052 0.23388961]\n",
      " [0.3609269  0.16324295 0.55153496 0.14852905]\n",
      " [0.30510392 0.19729795 0.68870884 0.11138469]]\n",
      " \n",
      " Shape of layer 1: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "### Multiply the values (i.e. \"np.dot(X,syn0)\" and then apply a sigmoid transformation: i.e. 1/(1+np.exp(-NUM))\n",
    "demo_hidden_layer  = 1/(1+np.exp(-(np.dot(X,syn0))))\n",
    "\n",
    "print(\"First version of node values in Hidden Layer: \\n  \\n{}\\n \\n Shape of layer 1: {}\".format(demo_hidden_layer, demo_hidden_layer.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate values of Output Layer (i.e. forward prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First version of node values in Output Layer: [[0.75297785]\n",
      " [0.77022754]\n",
      " [0.70504915]\n",
      " [0.72405614]]\n",
      " \n",
      " Shape of Output Layer: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "demo_output_layer = 1/(1+np.exp(-(np.dot(demo_hidden_layer,syn1))))\n",
    "\n",
    "\n",
    "print(\"First version of node values in Output Layer: {}\\n \\n Shape of Output Layer: {}\".format(demo_output_layer, demo_output_layer.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch processing: therefore wait until end of batch (i.e. all instances) to update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative of a sigmoid function:   (l2*(1-l2))  <br>\n",
    "<br>\n",
    "Convenient form for efficiently calculating gradients used in neural networks: if one keeps in memory the feed-forward activations of the logistic function for a given layer, the gradients for that layer can be evaluated using simple multiplication and subtraction rather than performing any re-evaluating the sigmoid function, which requires extra exponentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration 0: \n",
      " Distance from correct prediction: [[-0.75297785]\n",
      " [ 0.22977246]\n",
      " [ 0.29495085]\n",
      " [-0.72405614]]\n",
      "\n",
      " Iteration 6000: \n",
      " Distance from correct prediction: [[-0.01054519]\n",
      " [ 0.01319209]\n",
      " [ 0.01706785]\n",
      " [-0.01503702]]\n",
      "\n",
      " Iteration 12000: \n",
      " Distance from correct prediction: [[-0.00699539]\n",
      " [ 0.00853642]\n",
      " [ 0.01096649]\n",
      " [-0.0098401 ]]\n",
      "\n",
      " Iteration 18000: \n",
      " Distance from correct prediction: [[-0.00556491]\n",
      " [ 0.00665954]\n",
      " [ 0.0086165 ]\n",
      " [-0.00777354]]\n",
      "\n",
      " Iteration 24000: \n",
      " Distance from correct prediction: [[-0.00474119]\n",
      " [ 0.00558785]\n",
      " [ 0.00729363]\n",
      " [-0.00659081]]\n",
      "\n",
      " Iteration 30000: \n",
      " Distance from correct prediction: [[-0.00419021]\n",
      " [ 0.0048776 ]\n",
      " [ 0.00642058]\n",
      " [-0.00580231]]\n",
      "\n",
      " Iteration 36000: \n",
      " Distance from correct prediction: [[-0.00378921]\n",
      " [ 0.00436527]\n",
      " [ 0.00579071]\n",
      " [-0.00522965]]\n",
      "\n",
      " Iteration 42000: \n",
      " Distance from correct prediction: [[-0.00348102]\n",
      " [ 0.00397472]\n",
      " [ 0.00530948]\n",
      " [-0.00479016]]\n",
      "\n",
      " Iteration 48000: \n",
      " Distance from correct prediction: [[-0.00323489]\n",
      " [ 0.00366518]\n",
      " [ 0.00492679]\n",
      " [-0.00443956]]\n",
      "\n",
      " Iteration 54000: \n",
      " Distance from correct prediction: [[-0.00303265]\n",
      " [ 0.0034126 ]\n",
      " [ 0.00461333]\n",
      " [-0.00415175]]\n"
     ]
    }
   ],
   "source": [
    "### note: was xrange in Py2 - now it's range in Py3\n",
    "\n",
    "for j in range(60000):\n",
    "    hidden_layer  = 1/(1+np.exp(-(np.dot(X,syn0))))\n",
    "    output_layer = 1/(1+np.exp(-(np.dot(hidden_layer ,syn1))))\n",
    "    \n",
    "    ###############  (distance from correct answer)  X  (partial derivative of Sigmoid at this point)\n",
    "    l2_delta = (y - output_layer)*(output_layer*(1-output_layer))\n",
    "    \n",
    "    ###############  multiply how much we missed by the slope of the sigmoid at the values in l1\n",
    "    l1_delta = l2_delta.dot(syn1.T) * (hidden_layer * (1 - hidden_layer))\n",
    "    \n",
    "    ###############  update the weights.\n",
    "    syn1 = syn1 + hidden_layer.T.dot(l2_delta)\n",
    "    syn0 = syn0 + X.T.dot(l1_delta)\n",
    "    \n",
    "    if j % 6000 == 0:\n",
    "        print(\"\\n Iteration {}: \\n Distance from correct prediction: {}\".format(j, y - output_layer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights for features: \n",
      "[[-6.84907556 -7.22261104 -4.51861542 -0.72959157]\n",
      " [-4.9640562   6.24814052  7.44527005 -2.99103074]\n",
      " [ 1.93997688 -3.15757874  1.52582139  2.66809815]]\n",
      "\n",
      "Final weights for hidden layer to output: \n",
      "[[-7.29438893]\n",
      " [11.29629496]\n",
      " [-7.53537454]\n",
      " [ 6.68733369]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final weights for features: \\n{}\".format(syn0))\n",
    "print()\n",
    "print(\"Final weights for hidden layer to output: \\n{}\".format(syn1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
